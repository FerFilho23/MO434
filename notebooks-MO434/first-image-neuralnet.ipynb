{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of a first neural net on images, modified from Hands-On ML with Scikit-Learn, Keras & TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms, models, datasets\n",
    "from torchsummary import summary\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_snippets.torch_loader import Report\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "#is GPU available?\n",
    "gpu = torch.cuda.is_available()\n",
    "\n",
    "#defining device where to to the computation\n",
    "device = torch.device(0) if gpu else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's start by loading the training set of fashion MNIST. PyTorch has a number of functions to load popular datasets in 'torchvision.datasets'. The training set contains 60,000 grayscale images, each 28x28 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = datasets.FashionMNIST(root='./images',train=True,transform=None,download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize((0.5,),(0.5,),)   \n",
    "])\n",
    "\n",
    "\n",
    "class FMNistDataset():\n",
    "    def __init__(self, dataset, transform=None): \n",
    "        self.dataset   = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, target = self.dataset[index]\n",
    "        return self.transform(img), target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images are 28x28x1\n",
      "Class of the image:  Bag\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
    "\n",
    "dataset   = FMNistDataset(trainset,prep)\n",
    "(image, target) = dataset[100]\n",
    "nchannels = image.shape[0]\n",
    "height    = image.shape[1]\n",
    "width     = image.shape[2]\n",
    "image     = image.permute(1,2,0).numpy()\n",
    "image     = 255*(image - np.min(image))/(np.max(image)-np.min(image))\n",
    "image     = image.astype('uint8')\n",
    "print(\"Images are {}x{}x{}\".format(width,height,nchannels))\n",
    "plt.imshow(image)\n",
    "print(\"Class of the image: \", class_names[target])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's split our training set into training, validationm and test sets by creating one data loader for each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices      = list(range(len(dataset)))\n",
    "train_sample = SubsetRandomSampler(indices[:30000])\n",
    "valid_sample = SubsetRandomSampler(indices[30000:45000])\n",
    "test_sample  = SubsetRandomSampler(indices[45000:])\n",
    "trainloader  = DataLoader(dataset,sampler=train_sample,batch_size=64)\n",
    "validloader  = DataLoader(dataset,sampler=valid_sample,batch_size=64)\n",
    "testloader   = DataLoader(dataset,sampler=test_sample,batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create your first NN on images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this categorical image classification problem is simple, you may start by flattenning the input images into feature vectors to be the input of an MLP classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FMNistNet(nn.Module):\n",
    "\n",
    "    def __init__(self, input_shape, nclasses=10):\n",
    "        super(FMNistNet, self).__init__()\n",
    "        \n",
    "        #defining a MLP classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features=input_shape[1]*input_shape[2], out_features=300, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(in_features=300, out_features=100, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(100, nclasses)\n",
    "        )\n",
    "\n",
    "        #initialize weights\n",
    "        self.weights_init()\n",
    "\n",
    "    def forward(self, x):\n",
    "       #transform image features into a vector   \n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "\n",
    "        #classifies features\n",
    "        y = self.classifier(x)\n",
    "  \n",
    "        return y\n",
    "  \n",
    "    def weights_init(self):\n",
    "        for x in self.modules():\n",
    "            if isinstance(x, nn.Linear):\n",
    "                torch.nn.init.xavier_uniform_(x.weight.data)\n",
    "                if (x.bias is not None): \n",
    "                    x.bias.data.zero_()\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the model and define criterion and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FMNistNet(\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=300, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.25, inplace=False)\n",
      "    (3): Linear(in_features=300, out_features=100, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.25, inplace=False)\n",
      "    (6): Linear(in_features=100, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                  [-1, 300]         235,500\n",
      "              ReLU-2                  [-1, 300]               0\n",
      "           Dropout-3                  [-1, 300]               0\n",
      "            Linear-4                  [-1, 100]          30,100\n",
      "              ReLU-5                  [-1, 100]               0\n",
      "           Dropout-6                  [-1, 100]               0\n",
      "            Linear-7                   [-1, 10]           1,010\n",
      "================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 1.02\n",
      "Estimated Total Size (MB): 1.03\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def Criterion(preds, targets):\n",
    "    ce            = nn.CrossEntropyLoss().to(device)\n",
    "    loss          = ce(preds, targets.long()) \n",
    "    pred_labels   = torch.max(preds, 1)[1] # same as argmax\n",
    "    acc           = torch.sum(pred_labels == targets.data)/pred_labels.size(0)\n",
    "    return loss, acc\n",
    "    \n",
    "# Create the model with cross entropy loss and Adam optimizer\n",
    "model     = FMNistNet((nchannels,height,width)).to(device=device)\n",
    "print(model) # presents the architecture of the model\n",
    "criterion = Criterion\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "summary(model, (nchannels, height, width)) # presents the architecture of the model in more details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model and report the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(model, data, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    ims, targets = data\n",
    "    ims     = ims.to(device=device)\n",
    "    targets = targets.to(device=device)\n",
    "    preds   = model(ims)\n",
    "    loss, acc = criterion(preds, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    return loss.item(), acc.item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate_batch(model, data, criterion, device):\n",
    "    model.eval()\n",
    "    ims, targets = data\n",
    "    ims     = ims.to(device=device)\n",
    "    targets = targets.to(device=device)\n",
    "    preds   = model(ims)\n",
    "    loss, acc = criterion(preds, targets)\n",
    "    return loss.item(), acc.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 10.000  trn_loss: 0.735  val_acc: 0.806  trn_acc: 0.753  val_loss: 0.608  (38.77s - 116.31s remaining)\n",
      "EPOCH: 20.000  trn_loss: 0.584  val_acc: 0.836  trn_acc: 0.800  val_loss: 0.497  (77.95s - 77.95s remaining)\n",
      "EPOCH: 26.721  trn_loss: 0.444  trn_acc: 0.891  (107.66s - 53.50s remaining)"
     ]
    }
   ],
   "source": [
    "# initializing variables\n",
    "n_epochs = 40\n",
    "log      = Report(n_epochs)\n",
    "\n",
    "# starting training and validation\n",
    "for ex in range(n_epochs):\n",
    "    N = len(trainloader)\n",
    "    for bx, data in enumerate(trainloader):\n",
    "        loss, acc = train_batch(model, data, optimizer, criterion, device)\n",
    "        log.record((ex+(bx+1)/N), trn_loss=loss, trn_acc=acc, end='\\r')\n",
    "\n",
    "    N = len(validloader)\n",
    "    for bx, data in enumerate(validloader):\n",
    "        loss, acc = validate_batch(model, data, criterion, device)\n",
    "        log.record((ex+(bx+1)/N), val_loss=loss, val_acc=acc, end='\\r')\n",
    "\n",
    "    if (ex+1)%10==0: log.report_avgs(ex+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.plot_epochs(['trn_loss','val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.plot_epochs(['trn_acc','val_acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the NN on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(testloader)\n",
    "mean_loss = 0\n",
    "mean_acc  = 0\n",
    "for bx, data in enumerate(testloader):\n",
    "    loss, acc  = validate_batch(model, data, criterion, device)\n",
    "    mean_loss += loss\n",
    "    mean_acc  += acc\n",
    "    \n",
    "mean_loss = mean_loss / N\n",
    "mean_acc  = mean_acc / N\n",
    "print('Loss: {:.6f} Acc: {:.6f}'.format(mean_loss,mean_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.to('cpu').state_dict(), \"FMNistMLP.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: The model is underfitted, which might have to do with the complexity of the model and the proportions among the datasets' sizes. Play with the definition of the training, validation and test sets, network architecture, and hyperparameters to fix the problem without overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "nav_menu": {
   "height": "264px",
   "width": "369px"
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
